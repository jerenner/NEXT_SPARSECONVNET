{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test functions de building_blocks (no esta todo pero casi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from next_sparseconvnet.utils.data_loaders import DataGen, collatefn, LabelType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/home/mperez/NEXT_SPARSECONVNET/next_sparseconvnet/test_files/MC_dataset.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = DataGen(dataset_path, LabelType.Segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with datagen:\n",
    "    batch = [datagen[i] for i in range(3)]\n",
    "coord, ener, lab, ev = collatefn(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord, ener, lab, ev = collatefn(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = len(coord[0]) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([333, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coord.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import torch\n",
    "import sparseconvnet as scn\n",
    "\n",
    "from next_sparseconvnet.utils.data_loaders import DataGen, collatefn, LabelType\n",
    "from next_sparseconvnet.networks.building_blocks import * #luego aqui como el .py lo voy a guardar en la carpeta de networks creo que basta con poner . building_blocks\n",
    "\n",
    "def test_ResidualBlock_downsample(MCdataset):\n",
    "    datagen = DataGen(MCdataset, LabelType.Classification)\n",
    "    with datagen:\n",
    "        data = [datagen[i] for i in range(3)]\n",
    "    coord, ener, lab, ev = collatefn(data)\n",
    "    spatial_size = (50, 50, 50)\n",
    "    dim = 3\n",
    "    inplanes = 1\n",
    "    kernel = 2\n",
    "    stride = 2\n",
    "    \n",
    "    x = scn.InputLayer(dim, spatial_size)((coord, ener))\n",
    "    out = ResidualBlock_downsample(inplanes, kernel, stride)(x)\n",
    "    \n",
    "    assert out.features.shape[1] == 2 * inplanes\n",
    "    for i, size in enumerate(spatial_size):\n",
    "        assert out.spatial_size[i] == (size - kernel)/stride + 1\n",
    "\n",
    "        \n",
    "def test_ResidualBlock_basic(MCdataset):\n",
    "    datagen = DataGen(MCdataset, LabelType.Classification)\n",
    "    with datagen:\n",
    "        data = [datagen[i] for i in range(3)]\n",
    "    coord, ener, lab, ev = collatefn(data)\n",
    "    spatial_size = (50, 50, 50)\n",
    "    dim = 3\n",
    "    inplanes = 1\n",
    "    kernel = 2\n",
    "    \n",
    "    x = scn.InputLayer(dim, spatial_size)((coord, ener))\n",
    "    out = ResidualBlock_basic(inplanes, kernel)(x)\n",
    "    \n",
    "    assert out.features.shape[1] == inplanes\n",
    "    for i, size in enumerate(spatial_size):\n",
    "        assert out.spatial_size[i] == size\n",
    "        \n",
    "        \n",
    "def test_ResidualBlock_upsample(MCdataset):\n",
    "    datagen = DataGen(MCdataset, LabelType.Classification)\n",
    "    with datagen:\n",
    "        data = [datagen[i] for i in range(3)]\n",
    "    coord, ener, lab, ev = collatefn(data)\n",
    "    spatial_size = (50, 50, 50)\n",
    "    dim = 3\n",
    "    inplanes = 1\n",
    "    outplanes = 6\n",
    "    kernel = 2\n",
    "    stride = 2\n",
    "    \n",
    "    x = scn.InputLayer(dim, spatial_size)((coord, ener))\n",
    "    x = scn.SubmanifoldConvolution(dim, inplanes, outplanes, kernel, False)(x)\n",
    "    \n",
    "    inplanes = x.features.shape[1]\n",
    "    out = ResidualBlock_upsample(inplanes, kernel, stride)(x)\n",
    "    \n",
    "    assert out.features.shape[1] == inplanes / 2\n",
    "    for i, size in enumerate(spatial_size):\n",
    "        assert out.spatial_size[i] == kernel + stride * (size - 1)\n",
    "        \n",
    "        \n",
    "def test_ConvBNBlock(MCdataset):\n",
    "    datagen = DataGen(MCdataset, LabelType.Classification)\n",
    "    with datagen:\n",
    "        data = [datagen[i] for i in range(3)]\n",
    "    coord, ener, lab, ev = collatefn(data)\n",
    "    spatial_size = (50, 50, 50)\n",
    "    dim = 3\n",
    "    inplanes = 1\n",
    "    outplanes = 3\n",
    "    kernel = 2\n",
    "    stride = 2\n",
    "    \n",
    "    x = scn.InputLayer(dim, spatial_size)((coord, ener))\n",
    "    out = ConvBNBlock(inplanes, outplanes, kernel)(x)\n",
    "    out_with_stride = ConvBNBlock(inplanes, outplanes, kernel, stride = stride)(x)\n",
    "    \n",
    "    for i, size in enumerate(spatial_size):\n",
    "        assert out.spatial_size[i] == size\n",
    "        assert out_with_stride.spatial_size[i] == (size - kernel) / stride + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ResidualBlock_downsample(dataset_path)\n",
    "test_ResidualBlock_basic(dataset_path)\n",
    "test_ResidualBlock_upsample(dataset_path)\n",
    "test_ConvBNBlock(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test function de UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import torch\n",
    "import sparseconvnet as scn\n",
    "\n",
    "from next_sparseconvnet.utils.data_loaders import DataGen, collatefn, LabelType\n",
    "from next_sparseconvnet.networks.architectures import UNet\n",
    "\n",
    "\n",
    "def test_UNet(MCdataset):\n",
    "    datagen = DataGen(MCdataset, LabelType.Classification)\n",
    "    with datagen:\n",
    "        data = [datagen[i] for i in range(3)]\n",
    "    coord, ener, lab, ev = collatefn(data)\n",
    "    spatial_size = (51, 51, 51)\n",
    "    init_conv_nplanes = 4\n",
    "    init_conv_kernel = 3\n",
    "    kernel_sizes = [7, 5, 3]\n",
    "    stride = 2\n",
    "    basic_num = 3\n",
    "    nclasses = 3\n",
    "    \n",
    "    net = UNet(spatial_size, init_conv_nplanes, init_conv_kernel, kernel_sizes, stride, basic_num, nclasses = nclasses) \n",
    "    \n",
    "    last_basic = []\n",
    "    net.basic_up[0][2].add.register_forward_hook(lambda model, input, output: last_basic.append([output.spatial_size, output.features.shape]))\n",
    "        \n",
    "    assert len(net.downsample) == len(kernel_sizes) - 1\n",
    "    assert len(net.upsample)   == len(kernel_sizes) - 1\n",
    "    assert len(net.basic_down) == len(kernel_sizes) - 1\n",
    "    assert len(net.basic_up)   == len(kernel_sizes) - 1\n",
    "    assert len(net.basic_down[0]) == basic_num\n",
    "\n",
    "    out = net.forward((coord, ener))\n",
    "    \n",
    "    for i, size in enumerate(last_basic[0][0]):\n",
    "        assert size == spatial_size[i]\n",
    "    assert last_basic[0][1][1] == init_conv_nplanes\n",
    "    assert out.size()[0] == coord.size()[0]\n",
    "    assert out.size()[1] == nclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_UNet(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test functions de train_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from next_sparseconvnet.utils.data_loaders import DataGen, collatefn, LabelType #luego quitar todo pq este archivo esta en carpeta utils tb\n",
    "from next_sparseconvnet.networks.architectures import UNet\n",
    "from next_sparseconvnet.utils.train_utils import IoU, train_one_epoch_segmentation, valid_one_epoch_segmentation #aqui igual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/home/mperez/NEXT_SPARSECONVNET/next_sparseconvnet/test_files/MC_dataset.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_IoU():\n",
    "    a = [0, 1, 1, 2, 0, 0, 2]\n",
    "    b = [1, 1, 2, 0, 0, 1, 2]\n",
    "    iou_by_hand = [1/4, 1/4, 1/3]\n",
    "    iou = IoU(a, b)\n",
    "    np.testing.assert_allclose(iou, iou_by_hand)\n",
    "    \n",
    "    n = 5\n",
    "    iou = IoU(a, b, nclass = n)\n",
    "    assert len(iou) == n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_IoU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_one_epoch_segmentation(MCdataset):\n",
    "    datagen = DataGen(MCdataset, LabelType.Segmentation, nevents = 3)\n",
    "    loader = torch.utils.data.DataLoader(datagen, batch_size = 1, shuffle = True, num_workers=1, collate_fn=collatefn, drop_last=True, pin_memory=False)\n",
    "    \n",
    "    spatial_size = (51, 51, 51)\n",
    "    init_conv_nplanes = 4\n",
    "    init_conv_kernel = 3\n",
    "    kernel_sizes = [7, 5, 3]\n",
    "    stride_sizes = [2, 2]\n",
    "    basic_num = 3\n",
    "    nclasses = 3\n",
    "\n",
    "    net = UNet(spatial_size, init_conv_nplanes, init_conv_kernel, kernel_sizes, stride_sizes, basic_num, nclasses = nclasses)\n",
    "    net = net.cuda()\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss() \n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=1e-2, betas=(0.9, 0.999), eps=1e-6, weight_decay=0)\n",
    "    \n",
    "    train_one_epoch_segmentation(0, net, criterion, optimizer, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0\t Loss: 0.935048\n"
     ]
    }
   ],
   "source": [
    "test_train_one_epoch_segmentation(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_valid_one_epoch_segmentation(MCdataset):\n",
    "    datagen = DataGen(MCdataset, LabelType.Segmentation, nevents = 3)\n",
    "    loader = torch.utils.data.DataLoader(datagen, batch_size = 1, shuffle = True, num_workers=1, collate_fn=collatefn, drop_last=True, pin_memory=False)\n",
    "    \n",
    "    spatial_size = (51, 51, 51)\n",
    "    init_conv_nplanes = 4\n",
    "    init_conv_kernel = 3\n",
    "    kernel_sizes = [7, 5, 3]\n",
    "    stride_sizes = [2, 2]\n",
    "    basic_num = 3\n",
    "    nclasses = 3\n",
    "\n",
    "    net = UNet(spatial_size, init_conv_nplanes, init_conv_kernel, kernel_sizes, stride_sizes, basic_num, nclasses = nclasses)\n",
    "    net = net.cuda()\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss() \n",
    "    \n",
    "    valid_one_epoch_segmentation(net, criterion, loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Validation Loss: 1.052158\n"
     ]
    }
   ],
   "source": [
    "test_valid_one_epoch_segmentation(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
