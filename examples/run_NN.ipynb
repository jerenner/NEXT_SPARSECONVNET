{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "151ba95f",
   "metadata": {},
   "source": [
    "# Load, train and run (predict) a DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b89f7f",
   "metadata": {},
   "source": [
    "First, we are going to look in detail the process, and then we are going to show the function for a full train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6edb0b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from argparse     import ArgumentParser\n",
    "from argparse     import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44c460ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import tables as tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a46f18e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import invisible_cities.io.dst_io as dio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad79bfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from next_sparseconvnet.utils.data_loaders     import LabelType,       DataGen, collatefn, weights_loss\n",
    "from next_sparseconvnet.networks.architectures import NetArchitecture, UNet, ResNet\n",
    "from next_sparseconvnet.utils.train_utils      import *\n",
    "from next_sparseconvnet.utils.focal_loss       import FocalLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8837d77",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a41f893",
   "metadata": {},
   "source": [
    "Take the path of the train and validation files we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ae4d5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/lustre/ific.uv.es/ml/ific020/nexus_2021/train_dataset_5mm_all.h5\"\n",
    "valid_path = \"/lustre/ific.uv.es/ml/ific020/nexus_2021/valid_dataset_5mm.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b204451b",
   "metadata": {},
   "source": [
    "We can explore how do the files look like. With the new labelling files may have some differences, but not many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61a7a0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/lustre/ific.uv.es/ml/ific020/nexus_2021/train_dataset_5mm_all.h5 (File) ''\n",
      "Last modif.: 'Wed Jun  2 15:47:58 2021'\n",
      "Object Tree: \n",
      "/ (RootGroup) ''\n",
      "/DATASET (Group) ''\n",
      "/DATASET/BinsInfo (Table(1,), shuffle, zlib(4)) ''\n",
      "/DATASET/EventsInfo (Table(673706,), shuffle, zlib(4)) ''\n",
      "/DATASET/Voxels (Table(34817578,), shuffle, zlib(4)) ''\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tb.open_file(train_path, 'r') as h5in:\n",
    "    print(h5in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fface6e",
   "metadata": {},
   "source": [
    "First we have the bin information of the data, so we can see how the detector is discretized. This data is for NEXT-White."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d630dd0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_x</th>\n",
       "      <th>max_x</th>\n",
       "      <th>nbins_x</th>\n",
       "      <th>min_y</th>\n",
       "      <th>max_y</th>\n",
       "      <th>nbins_y</th>\n",
       "      <th>min_z</th>\n",
       "      <th>max_z</th>\n",
       "      <th>nbins_z</th>\n",
       "      <th>Rmax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-220</td>\n",
       "      <td>220</td>\n",
       "      <td>89</td>\n",
       "      <td>-220</td>\n",
       "      <td>220</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>550</td>\n",
       "      <td>111</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   min_x  max_x  nbins_x  min_y  max_y  nbins_y  min_z  max_z  nbins_z  Rmax\n",
       "0   -220    220       89   -220    220       89      0    550      111   220"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dio.load_dst(train_path, 'DATASET', 'BinsInfo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273ddbd0",
   "metadata": {},
   "source": [
    "Then we have a mapping between the events here contained and the files where each event came from. \n",
    "\n",
    "Originally, the MC events were distributed within several files (basename) with a specific identificator (event_id), and when we prepared the dataset, we joint all of them so we gave them a new unique identifyer (dataset_id), as event_id was repeated along different files. \n",
    "\n",
    "Also, it has per event the binary class (binclass) information, that tells us whether the event is background-0, or signal-1. \n",
    "\n",
    "The pathname was the rest of the directory name for the MC files, but in gpu1. In artemisa, the pathname is '/lustre/ific.uv.es/ml/ific020/nexus_2021/MC_production'. Having the MC information is useful to check what happened in an specific event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e207753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>binclass</th>\n",
       "      <th>pathname</th>\n",
       "      <th>basename</th>\n",
       "      <th>dataset_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000000</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/gdiaz/data_for_marija/tlde_nn/nexus</td>\n",
       "      <td>nexus_100_tlde.h5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100000001</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/gdiaz/data_for_marija/tlde_nn/nexus</td>\n",
       "      <td>nexus_100_tlde.h5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100000002</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/gdiaz/data_for_marija/tlde_nn/nexus</td>\n",
       "      <td>nexus_100_tlde.h5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100000003</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/gdiaz/data_for_marija/tlde_nn/nexus</td>\n",
       "      <td>nexus_100_tlde.h5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100000004</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/gdiaz/data_for_marija/tlde_nn/nexus</td>\n",
       "      <td>nexus_100_tlde.h5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673701</th>\n",
       "      <td>1999000337</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/gdiaz/data_for_marija/tlde_nn/nexus</td>\n",
       "      <td>nexus_1999_tlde.h5</td>\n",
       "      <td>644240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673702</th>\n",
       "      <td>1999000338</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/gdiaz/data_for_marija/tlde_nn/nexus</td>\n",
       "      <td>nexus_1999_tlde.h5</td>\n",
       "      <td>644241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673703</th>\n",
       "      <td>1999000339</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/gdiaz/data_for_marija/tlde_nn/nexus</td>\n",
       "      <td>nexus_1999_tlde.h5</td>\n",
       "      <td>644242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673704</th>\n",
       "      <td>1999000340</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/gdiaz/data_for_marija/tlde_nn/nexus</td>\n",
       "      <td>nexus_1999_tlde.h5</td>\n",
       "      <td>644243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673705</th>\n",
       "      <td>1999000341</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/gdiaz/data_for_marija/tlde_nn/nexus</td>\n",
       "      <td>nexus_1999_tlde.h5</td>\n",
       "      <td>644244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>673706 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          event_id  binclass                                   pathname  \\\n",
       "0        100000000         0  /home/gdiaz/data_for_marija/tlde_nn/nexus   \n",
       "1        100000001         0  /home/gdiaz/data_for_marija/tlde_nn/nexus   \n",
       "2        100000002         1  /home/gdiaz/data_for_marija/tlde_nn/nexus   \n",
       "3        100000003         0  /home/gdiaz/data_for_marija/tlde_nn/nexus   \n",
       "4        100000004         0  /home/gdiaz/data_for_marija/tlde_nn/nexus   \n",
       "...            ...       ...                                        ...   \n",
       "673701  1999000337         1  /home/gdiaz/data_for_marija/tlde_nn/nexus   \n",
       "673702  1999000338         0  /home/gdiaz/data_for_marija/tlde_nn/nexus   \n",
       "673703  1999000339         0  /home/gdiaz/data_for_marija/tlde_nn/nexus   \n",
       "673704  1999000340         1  /home/gdiaz/data_for_marija/tlde_nn/nexus   \n",
       "673705  1999000341         0  /home/gdiaz/data_for_marija/tlde_nn/nexus   \n",
       "\n",
       "                  basename  dataset_id  \n",
       "0        nexus_100_tlde.h5           0  \n",
       "1        nexus_100_tlde.h5           1  \n",
       "2        nexus_100_tlde.h5           2  \n",
       "3        nexus_100_tlde.h5           3  \n",
       "4        nexus_100_tlde.h5           4  \n",
       "...                    ...         ...  \n",
       "673701  nexus_1999_tlde.h5      644240  \n",
       "673702  nexus_1999_tlde.h5      644241  \n",
       "673703  nexus_1999_tlde.h5      644242  \n",
       "673704  nexus_1999_tlde.h5      644243  \n",
       "673705  nexus_1999_tlde.h5      644244  \n",
       "\n",
       "[673706 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dio.load_dst(train_path, 'DATASET', 'EventsInfo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ab5c42",
   "metadata": {},
   "source": [
    "Finally we have the voxels for each event, with position, energy and the segmentation class (segclass), which is 0-other particles, 1-track, 2-blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a05a7d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xbin</th>\n",
       "      <th>ybin</th>\n",
       "      <th>zbin</th>\n",
       "      <th>energy</th>\n",
       "      <th>segclass</th>\n",
       "      <th>binclass</th>\n",
       "      <th>dataset_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47</td>\n",
       "      <td>45</td>\n",
       "      <td>74</td>\n",
       "      <td>0.028918</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43</td>\n",
       "      <td>48</td>\n",
       "      <td>78</td>\n",
       "      <td>0.043978</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>45</td>\n",
       "      <td>81</td>\n",
       "      <td>0.012179</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49</td>\n",
       "      <td>46</td>\n",
       "      <td>74</td>\n",
       "      <td>0.005277</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43</td>\n",
       "      <td>48</td>\n",
       "      <td>77</td>\n",
       "      <td>0.009145</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34817573</th>\n",
       "      <td>19</td>\n",
       "      <td>33</td>\n",
       "      <td>68</td>\n",
       "      <td>0.049901</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>644244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34817574</th>\n",
       "      <td>19</td>\n",
       "      <td>31</td>\n",
       "      <td>68</td>\n",
       "      <td>0.018297</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>644244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34817575</th>\n",
       "      <td>19</td>\n",
       "      <td>33</td>\n",
       "      <td>69</td>\n",
       "      <td>0.005991</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>644244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34817576</th>\n",
       "      <td>16</td>\n",
       "      <td>35</td>\n",
       "      <td>70</td>\n",
       "      <td>0.032205</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>644244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34817577</th>\n",
       "      <td>15</td>\n",
       "      <td>34</td>\n",
       "      <td>70</td>\n",
       "      <td>0.044786</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>644244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34817578 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          xbin  ybin  zbin    energy  segclass  binclass  dataset_id\n",
       "0           47    45    74  0.028918         1         0           0\n",
       "1           43    48    78  0.043978         1         0           0\n",
       "2           41    45    81  0.012179         1         0           0\n",
       "3           49    46    74  0.005277         1         0           0\n",
       "4           43    48    77  0.009145         1         0           0\n",
       "...        ...   ...   ...       ...       ...       ...         ...\n",
       "34817573    19    33    68  0.049901         1         0      644244\n",
       "34817574    19    31    68  0.018297         1         0      644244\n",
       "34817575    19    33    69  0.005991         1         0      644244\n",
       "34817576    16    35    70  0.032205         1         0      644244\n",
       "34817577    15    34    70  0.044786         1         0      644244\n",
       "\n",
       "[34817578 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dio.load_dst(train_path, 'DATASET', 'Voxels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8093aa8",
   "metadata": {},
   "source": [
    "We now choose one of the two ways to read the data (either for classification or for segmentation) and the amount of data we want to load. Also, we can decide whether or not to perform data augmentation (rotating and mirroring data randomly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be2a8547",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_type = LabelType.Classification\n",
    "#label_type = LabelType.Segmentation\n",
    "\n",
    "#we use few data as it is a test, more data would become very heavy\n",
    "nevents_train = 100 \n",
    "nevents_valid = 10\n",
    "\n",
    "augmentation = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b321a710",
   "metadata": {},
   "source": [
    "And create the Data Generator for train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b03e562",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_train = DataGen(train_path, label_type, nevents = nevents_train, augmentation = augmentation)\n",
    "datagen_valid = DataGen(valid_path, label_type, nevents = nevents_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4e0684",
   "metadata": {},
   "source": [
    "The Data Generator stores the data in the way that the NN needs. As we see in the next cell, picking an element returns a tuple with arrays (xbin, ybin, zbin, energy, binclass/segclass, dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5704f454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([53, 68, 50, 54, 57, 49, 50, 56, 53, 52, 50, 54, 50, 83, 52, 60, 57,\n",
       "        60, 57, 50, 55, 51, 49, 53, 51, 51, 60, 52, 53, 52, 51, 51, 60, 60,\n",
       "        52, 52, 68, 60, 57, 57, 53, 51, 60, 50, 51, 52, 58, 59]),\n",
       " array([22, 11, 21, 22, 19, 22, 22, 19, 19, 19, 22, 19, 22, 48, 19, 20, 20,\n",
       "        20, 20, 21, 19, 22, 22, 21, 20, 20, 21, 20, 21, 22, 21, 21, 22, 21,\n",
       "        20, 18, 10, 19, 19, 19, 18, 22, 22, 20, 22, 18, 19, 19]),\n",
       " array([85, 94, 82, 85, 86, 83, 84, 86, 86, 84, 83, 86, 82, 70, 80, 89, 87,\n",
       "        88, 86, 81, 86, 81, 82, 84, 80, 81, 88, 80, 85, 84, 82, 81, 89, 89,\n",
       "        84, 84, 84, 89, 87, 88, 85, 82, 88, 81, 84, 85, 89, 89]),\n",
       " array([0.03313701, 0.0343953 , 0.12476786, 0.00629492, 0.02176605,\n",
       "        0.01348927, 0.01821891, 0.03875297, 0.02572587, 0.03190605,\n",
       "        0.02469233, 0.02831793, 0.021687  , 0.0018993 , 0.0084107 ,\n",
       "        0.03359486, 0.0260051 , 0.01074126, 0.0081438 , 0.03462485,\n",
       "        0.03355365, 0.04162054, 0.00458728, 0.00430069, 0.09175134,\n",
       "        0.05023536, 0.04170904, 0.00626328, 0.03558329, 0.03540052,\n",
       "        0.0550744 , 0.05767269, 0.17529312, 0.01078655, 0.02575558,\n",
       "        0.02820876, 0.02816699, 0.03863931, 0.01665314, 0.03796703,\n",
       "        0.03225745, 0.04136236, 0.08302112, 0.02292054, 0.02608812,\n",
       "        0.01153739, 0.04364327, 0.03034398]),\n",
       " array([1]),\n",
       " 14)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datagen_train[14]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4df7d93",
   "metadata": {},
   "source": [
    "Now we also pick the size of the batch. A batch is a group of events that is passed through the NN together; it is a common practice to divide all the data into these groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5936b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 20\n",
    "batch_size_valid = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a764ff1",
   "metadata": {},
   "source": [
    "And create the Data Loader, which is the object that will pass through the net with all the information transformed into torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48a2a581",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = torch.utils.data.DataLoader(datagen_train, batch_size = batch_size_train, shuffle = True, num_workers=1, collate_fn=collatefn, drop_last=True, pin_memory=False)\n",
    "loader_valid = torch.utils.data.DataLoader(datagen_valid, batch_size = batch_size_valid, shuffle = True, num_workers=1, collate_fn=collatefn, drop_last=True, pin_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c02d62e",
   "metadata": {},
   "source": [
    "For example, with 100 events and a batch size of 20, we will have 5 batches. \n",
    "\n",
    "If we have n_voxels for N events, the batch will be made of a tuple with size [n_voxels, 4] for the coordinates (xbin, ybin, zbin, id); [n_voxels, 1] for the energy; [N] or [n_voxels] with the binclass or segmentation and [N] with the dataset_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03d94275",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1037, 4]) torch.Size([1037, 1]) torch.Size([20]) torch.Size([20])\n",
      "torch.Size([1095, 4]) torch.Size([1095, 1]) torch.Size([20]) torch.Size([20])\n",
      "torch.Size([1039, 4]) torch.Size([1039, 1]) torch.Size([20]) torch.Size([20])\n",
      "torch.Size([977, 4]) torch.Size([977, 1]) torch.Size([20]) torch.Size([20])\n",
      "torch.Size([1057, 4]) torch.Size([1057, 1]) torch.Size([20]) torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "for batch in loader_train:\n",
    "    print(batch[0].size(), batch[1].size(), batch[2].size(), batch[3].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f60ff85",
   "metadata": {},
   "source": [
    "We can see here the batch's dataset_ids, which are randomly picked using the parametre shuffle in the Data Loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cb7f9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([68, 60, 70, 36, 44,  9, 80, 37, 71, 74, 35, 11, 73, 94, 46, 57, 99, 85,\n",
       "        25, 51], dtype=torch.int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589504cb",
   "metadata": {},
   "source": [
    "As a summary, the needed parameters for the loading of the data are:\n",
    "\n",
    "* label type (Classification/Segmentation)\n",
    "* number of events\n",
    "* batch size\n",
    "* augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8b7917",
   "metadata": {},
   "source": [
    "## Create the net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9755d7fe",
   "metadata": {},
   "source": [
    "Then, we create the net as an object; we have two different nets, but as we know, the major part of the structure is similar, so the needed parameters will be pretty much the same excepting a couple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "104c4ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMMON NET PARAMS\n",
    "spatial_size      = (543, 543, 543)\n",
    "init_conv_nplanes = 8\n",
    "init_conv_kernel  = 7\n",
    "kernel_sizes      = [7, 7, 5, 3, 3, 3]\n",
    "stride_sizes      = [4, 2, 2, 2, 2]\n",
    "basic_num         = 2\n",
    "momentum          = 0.7\n",
    "\n",
    "#RESNET\n",
    "nlinear           = 32\n",
    "\n",
    "#UNET\n",
    "nclasses          = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb926d3b",
   "metadata": {},
   "source": [
    "The explanation of the parameters is found in the tutorial.\n",
    "\n",
    "We load the net simply as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e77039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLASSIFICATION\n",
    "net = ResNet(spatial_size,\n",
    "             init_conv_nplanes,\n",
    "             init_conv_kernel,\n",
    "             kernel_sizes,\n",
    "             stride_sizes,\n",
    "             basic_num,\n",
    "             momentum = momentum,\n",
    "             nlinear  = nlinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7478fad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEGMENTATION\n",
    "#net = UNet(spatial_size,\n",
    "#           init_conv_nplanes,\n",
    "#           init_conv_kernel,\n",
    "#           kernel_sizes,\n",
    "#           stride_sizes,\n",
    "#           basic_num,\n",
    "#           momentum = momentum, \n",
    "#           nclasses = nclasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfff7c7",
   "metadata": {},
   "source": [
    "In the practice, the architecture is chosen usingn the netarch param, and it can be either NetArchitecture.ResNet or NetArchitecture.UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796c1f4e",
   "metadata": {},
   "source": [
    "We dont use cuda in the notebook because i couldn't make it work, i think because artemisa limits the gpu cuda usage to only sending jobs or trying them with an interactive session of 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df9e2bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f763b0f",
   "metadata": {},
   "source": [
    "We can see the layers and configuration of the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3b1678f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (inp): InputLayer()\n",
       "  (convBN): ConvBNBlock(\n",
       "    (conv): SubmanifoldConvolution 1->8 C7\n",
       "    (bnr): BatchNormReLU(8,eps=0.0001,momentum=0.7,affine=True)\n",
       "  )\n",
       "  (downsample): ModuleList(\n",
       "    (0): ResidualBlock_downsample(\n",
       "      (bnr1): BatchNormReLU(8,eps=0.0001,momentum=0.7,affine=True)\n",
       "      (conv1): Convolution 8->16 C7/4\n",
       "      (bnr2): BatchNormReLU(16,eps=0.0001,momentum=0.7,affine=True)\n",
       "      (subconv): SubmanifoldConvolution 16->16 C7\n",
       "      (conv2): Convolution 8->16 C7/4\n",
       "      (add): AddTable()\n",
       "    )\n",
       "    (1): ResidualBlock_downsample(\n",
       "      (bnr1): BatchNormReLU(16,eps=0.0001,momentum=0.7,affine=True)\n",
       "      (conv1): Convolution 16->32 C7/2\n",
       "      (bnr2): BatchNormReLU(32,eps=0.0001,momentum=0.7,affine=True)\n",
       "      (subconv): SubmanifoldConvolution 32->32 C7\n",
       "      (conv2): Convolution 16->32 C7/2\n",
       "      (add): AddTable()\n",
       "    )\n",
       "    (2): ResidualBlock_downsample(\n",
       "      (bnr1): BatchNormReLU(32,eps=0.0001,momentum=0.7,affine=True)\n",
       "      (conv1): Convolution 32->64 C5/2\n",
       "      (bnr2): BatchNormReLU(64,eps=0.0001,momentum=0.7,affine=True)\n",
       "      (subconv): SubmanifoldConvolution 64->64 C5\n",
       "      (conv2): Convolution 32->64 C5/2\n",
       "      (add): AddTable()\n",
       "    )\n",
       "    (3): ResidualBlock_downsample(\n",
       "      (bnr1): BatchNormReLU(64,eps=0.0001,momentum=0.7,affine=True)\n",
       "      (conv1): Convolution 64->128 C3/2\n",
       "      (bnr2): BatchNormReLU(128,eps=0.0001,momentum=0.7,affine=True)\n",
       "      (subconv): SubmanifoldConvolution 128->128 C3\n",
       "      (conv2): Convolution 64->128 C3/2\n",
       "      (add): AddTable()\n",
       "    )\n",
       "    (4): ResidualBlock_downsample(\n",
       "      (bnr1): BatchNormReLU(128,eps=0.0001,momentum=0.7,affine=True)\n",
       "      (conv1): Convolution 128->256 C3/2\n",
       "      (bnr2): BatchNormReLU(256,eps=0.0001,momentum=0.7,affine=True)\n",
       "      (subconv): SubmanifoldConvolution 256->256 C3\n",
       "      (conv2): Convolution 128->256 C3/2\n",
       "      (add): AddTable()\n",
       "    )\n",
       "  )\n",
       "  (basic): ModuleList(\n",
       "    (0): ModuleList(\n",
       "      (0): ResidualBlock_basic(\n",
       "        (bnr1): BatchNormReLU(8,eps=0.0001,momentum=0.7,affine=True)\n",
       "        (subconv1): SubmanifoldConvolution 8->8 C7\n",
       "        (bnr2): BatchNormReLU(8,eps=0.0001,momentum=0.7,affine=True)\n",
       "        (subconv2): SubmanifoldConvolution 8->8 C7\n",
       "        (add): AddTable()\n",
       "      )\n",
       "      (1): ResidualBlock_basic(\n",
       "        (bnr1): BatchNormReLU(8,eps=0.0001,momentum=0.7,affine=True)\n",
       "        (subconv1): SubmanifoldConvolution 8->8 C7\n",
       "        (bnr2): BatchNormReLU(8,eps=0.0001,momentum=0.7,affine=True)\n",
       "        (subconv2): SubmanifoldConvolution 8->8 C7\n",
       "        (add): AddTable()\n",
       "      )\n",
       "    )\n",
       "    (1): ModuleList(\n",
       "      (0): ResidualBlock_basic(\n",
       "        (bnr1): BatchNormReLU(16,eps=0.0001,momentum=0.7,affine=True)\n",
       "        (subconv1): SubmanifoldConvolution 16->16 C7\n",
       "        (bnr2): BatchNormReLU(16,eps=0.0001,momentum=0.7,affine=True)\n",
       "        (subconv2): SubmanifoldConvolution 16->16 C7\n",
       "        (add): AddTable()\n",
       "      )\n",
       "      (1): ResidualBlock_basic(\n",
       "        (bnr1): BatchNormReLU(16,eps=0.0001,momentum=0.7,affine=True)\n",
       "        (subconv1): SubmanifoldConvolution 16->16 C7\n",
       "        (bnr2): BatchNormReLU(16,eps=0.0001,momentum=0.7,affine=True)\n",
       "        (subconv2): SubmanifoldConvolution 16->16 C7\n",
       "        (add): AddTable()\n",
       "      )\n",
       "    )\n",
       "    (2): ModuleList(\n",
       "      (0): ResidualBlock_basic(\n",
       "        (bnr1): BatchNormReLU(32,eps=0.0001,momentum=0.7,affine=True)\n",
       "        (subconv1): SubmanifoldConvolution 32->32 C5\n",
       "        (bnr2): BatchNormReLU(32,eps=0.0001,momentum=0.7,affine=True)\n",
       "        (subconv2): SubmanifoldConvolution 32->32 C5\n",
       "        (add): AddTable()\n",
       "      )\n",
       "      (1): ResidualBlock_basic(\n",
       "        (bnr1): BatchNormReLU(32,eps=0.0001,momentum=0.7,affine=True)\n",
       "        (subconv1): SubmanifoldConvolution 32->32 C5\n",
       "        (bnr2): BatchNormReLU(32,eps=0.0001,momentum=0.7,affine=True)\n",
       "        (subconv2): SubmanifoldConvolution 32->32 C5\n",
       "        (add): AddTable()\n",
       "      )\n",
       "    )\n",
       "    (3): ModuleList(\n",
       "      (0): ResidualBlock_basic(\n",
       "        (bnr1): BatchNormReLU(64,eps=0.0001,momentum=0.7,affine=True)\n",
       "        (subconv1): SubmanifoldConvolution 64->64 C3\n",
       "        (bnr2): BatchNormReLU(64,eps=0.0001,momentum=0.7,affine=True)\n",
       "        (subconv2): SubmanifoldConvolution 64->64 C3\n",
       "        (add): AddTable()\n",
       "      )\n",
       "      (1): ResidualBlock_basic(\n",
       "        (bnr1): BatchNormReLU(64,eps=0.0001,momentum=0.7,affine=True)\n",
       "        (subconv1): SubmanifoldConvolution 64->64 C3\n",
       "        (bnr2): BatchNormReLU(64,eps=0.0001,momentum=0.7,affine=True)\n",
       "        (subconv2): SubmanifoldConvolution 64->64 C3\n",
       "        (add): AddTable()\n",
       "      )\n",
       "    )\n",
       "    (4): ModuleList(\n",
       "      (0): ResidualBlock_basic(\n",
       "        (bnr1): BatchNormReLU(128,eps=0.0001,momentum=0.7,affine=True)\n",
       "        (subconv1): SubmanifoldConvolution 128->128 C3\n",
       "        (bnr2): BatchNormReLU(128,eps=0.0001,momentum=0.7,affine=True)\n",
       "        (subconv2): SubmanifoldConvolution 128->128 C3\n",
       "        (add): AddTable()\n",
       "      )\n",
       "      (1): ResidualBlock_basic(\n",
       "        (bnr1): BatchNormReLU(128,eps=0.0001,momentum=0.7,affine=True)\n",
       "        (subconv1): SubmanifoldConvolution 128->128 C3\n",
       "        (bnr2): BatchNormReLU(128,eps=0.0001,momentum=0.7,affine=True)\n",
       "        (subconv2): SubmanifoldConvolution 128->128 C3\n",
       "        (add): AddTable()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bottom): ModuleList(\n",
       "    (0): ResidualBlock_basic(\n",
       "      (bnr1): BatchNormReLU(256,eps=0.0001,momentum=0.7,affine=True)\n",
       "      (subconv1): SubmanifoldConvolution 256->256 C3\n",
       "      (bnr2): BatchNormReLU(256,eps=0.0001,momentum=0.7,affine=True)\n",
       "      (subconv2): SubmanifoldConvolution 256->256 C3\n",
       "      (add): AddTable()\n",
       "    )\n",
       "    (1): ResidualBlock_basic(\n",
       "      (bnr1): BatchNormReLU(256,eps=0.0001,momentum=0.7,affine=True)\n",
       "      (subconv1): SubmanifoldConvolution 256->256 C3\n",
       "      (bnr2): BatchNormReLU(256,eps=0.0001,momentum=0.7,affine=True)\n",
       "      (subconv2): SubmanifoldConvolution 256->256 C3\n",
       "      (add): AddTable()\n",
       "    )\n",
       "  )\n",
       "  (max): MaxPooling7/1\n",
       "  (sparse): SparseToDense(3,256)\n",
       "  (linear1): Linear(in_features=256, out_features=32, bias=True)\n",
       "  (linear2): Linear(in_features=32, out_features=2, bias=True)\n",
       "  (activation): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8768dc",
   "metadata": {},
   "source": [
    "## Choose and configure CRITERION and OPTIMIZER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fa7499",
   "metadata": {},
   "source": [
    "The criterion quantifies how good the NN is performing on the fly. We have used the Cross Entropy Loss, which is a well extended loss function, and also the Focal Loss, which gave very good results for the semantic segmentation issue, as it is specific for segmentation problems.\n",
    "\n",
    "The optimizer updates the weights of the different layers in the NN, searching for a minimum in the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb834ac9",
   "metadata": {},
   "source": [
    "### CRITERION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e361d698",
   "metadata": {},
   "source": [
    "The criterion will require some weights to give more value to some classes than others. For Classification, we need 2 weights as there are 2 classes, and for Segmentation 3. \n",
    "\n",
    "In the configs there are two parameters related to this:\n",
    "* LossType - can be 'CrossEntropyLoss' or 'FocalLoss' (the latter is more suitable for Segmentation)\n",
    "* weights_loss - if None, there are no weights used; if True, weights are calculated using inverse frequencies of each class in the data; also a list of values can be specified. For the Focal Loss these weights are a parameter alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72b8ee7",
   "metadata": {},
   "source": [
    "For example, for the label type chosen before in the notebook, we can compute with the following function those weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b6b4d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71442885 0.28557115]\n"
     ]
    }
   ],
   "source": [
    "weights_loss_values = weights_loss(train_path, 10000, label_type, effective_number=False)\n",
    "print(weights_loss_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c267a77c",
   "metadata": {},
   "source": [
    "And we can create our criterion; if we chose CrossEntropyLoss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5aebf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(weight = torch.Tensor(weights_loss_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239c8b80",
   "metadata": {},
   "source": [
    "If we are using Segmentation and chose FocalLoss, this loss was implemented by hand (Cross Entropy Loss comes with torch) and the optimal parameters, as in the paper (arXiv:1708.02002) would be: alpha = 0.25 (per class) and gamma = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "325581d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This would be for segmentation, as alpha is a list with 3 elements.\n",
    "#criterion = FocalLoss(alpha=[0.25, 0.25, 0.25], gamma=2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54551391",
   "metadata": {},
   "source": [
    "### OPTIMIZER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e4e00d",
   "metadata": {},
   "source": [
    "The optimizer will require the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e954f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "betas = (0.9, 0.999)\n",
    "eps = 1e-6\n",
    "weight_decay = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb322fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()),\n",
    "                                     lr = lr,\n",
    "                                     betas = betas,\n",
    "                                     eps = eps,\n",
    "                                     weight_decay = weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbf9eda",
   "metadata": {},
   "source": [
    "## TRAIN&VALID ONE EPOCH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4ce1ef",
   "metadata": {},
   "source": [
    "Now that we have the data loaded, the net built, the criterion and the optimizer chosen and configured, we can train the net. \n",
    "\n",
    "The following function trains the net passing through all the used data once: this is an epoch. In a full training, you usually train for several epoches, and each time the net is expected to have better results. \n",
    "\n",
    "This function prints the number of the epoch, and the loss computed with the chosen criterion, and also returns the value of the loss and the value of the computed metric. For Classificacion, we use the regular accuracy, and for Segmentation we use IoU.\n",
    "\n",
    "I've added the False use_cuda argument because as I said, I couldn't get cuda to work in jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "065848a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/ific.uv.es/ml/ific020/software/SparseConvNet/sparseconvnet/convolution.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  (input.spatial_size - self.filter_size) // self.filter_stride + 1\n",
      "/lustre/ific.uv.es/ml/ific020/software/SparseConvNet/sparseconvnet/maxPooling.py:83: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  input.spatial_size - self.pool_size) // self.pool_stride + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0\t Loss: 1.456046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.4560464382171632, tensor(0.6400))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_one_epoch(0, net, criterion, optimizer, loader_train, label_type, use_cuda = False)\n",
    "\n",
    "#the warning is for something that SparseConvNet hasn't updated and PyTorch complains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65517a0",
   "metadata": {},
   "source": [
    "During the training is also important to cross check the performance of the net with some different data o avoid over training, so there is an analogue function for validation, which is used right after each epoch train. The main difference between them is that in the train function, the net weights are updated and the net ''improves'', while for the validation function we just evaluate the net and compute the loss etc, without using the optimizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a45bee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Validation Loss: 4.260647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4.26064670085907, tensor(0.7100))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_one_epoch(net, criterion, loader_train, label_type, use_cuda = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae45efb1",
   "metadata": {},
   "source": [
    "## TRAIN NET COMLPETELY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f27ae84",
   "metadata": {},
   "source": [
    "So, the train_net function used in the main.py script what does is, for a certain number of epoches:\n",
    "* Loads data\n",
    "* Trains and validates an epoch of the loaded data with the net, criterion and optimizer chosen\n",
    "* Saves a checkpoint of the net (similar to a photograph of the current status) if the validation loss improved with respect to the previous epoch. If the loss of an epoch starts raising it is an indicator of overtraining. This checkpoint is used either to continue a train starting from an already trained net, or to use the net to get results!\n",
    "* Saves the metrics and loss for train and validation data, to explore the performance of the whole train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017df2f6",
   "metadata": {},
   "source": [
    "Then, we need some new parameters:\n",
    "* nepoch - the number of epochs we want to train\n",
    "* checkpoint_dir - directory to save the checkpoints\n",
    "* tensorboard_dir - directory to save the metrics and loss (there is a notebook called read_tensorboard_artemisa which explains how to read these files; then also there is an interactive option using in artemisa the command ''tensorboard --logdir=/path/to/tensorboard/file''\n",
    "* num_workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b1fee7",
   "metadata": {},
   "source": [
    "I created a folder in examples to store the checkpoints and tensorboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3063ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nepoch = 3\n",
    "checkpoint_dir = '/lhome/ext/ific020/ific0201/NEXT_SPARSECONVNET/examples/net_savings/checkpoints'\n",
    "tensorboard_dir = '/lhome/ext/ific020/ific0201/NEXT_SPARSECONVNET/examples/net_savings/logs'\n",
    "num_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40507504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/ific.uv.es/ml/ific020/software/SparseConvNet/sparseconvnet/convolution.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  (input.spatial_size - self.filter_size) // self.filter_stride + 1\n",
      "/lustre/ific.uv.es/ml/ific020/software/SparseConvNet/sparseconvnet/maxPooling.py:83: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  input.spatial_size - self.pool_size) // self.pool_stride + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0\t Loss: 0.724674\n",
      "\t Validation Loss: 0.632538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/ific.uv.es/ml/ific020/software/miniconda/envs/IC-3.7-2020-06-16/lib/python3.7/site-packages/torch/nn/modules/module.py:1384: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  \" and \".join(warn_msg) + \" are deprecated. nn.Module.state_dict will not accept them in the future. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1\t Loss: 0.575022\n",
      "\t Validation Loss: 0.571277\n",
      "Train Epoch: 2\t Loss: 0.731007\n",
      "\t Validation Loss: 0.530173\n"
     ]
    }
   ],
   "source": [
    "train_net(nepoch = nepoch,\n",
    "          train_data_path = train_path,\n",
    "          valid_data_path = valid_path,\n",
    "          train_batch_size = batch_size_train,\n",
    "          valid_batch_size = batch_size_valid,\n",
    "          net = net,\n",
    "          label_type = label_type,\n",
    "          criterion = criterion,\n",
    "          optimizer = optimizer,\n",
    "          checkpoint_dir = checkpoint_dir,\n",
    "          tensorboard_dir = tensorboard_dir,\n",
    "          num_workers = num_workers,\n",
    "          nevents_train = nevents_train,\n",
    "          nevents_valid = nevents_valid,\n",
    "          augmentation  = augmentation, \n",
    "          use_cuda = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615c0ec9",
   "metadata": {},
   "source": [
    "Now we have a trained net, after some epochs! We can choose to keep on training it more epochs, just passing the net through the function again, changing things trying to improve, or we can use it now to predict!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a95029",
   "metadata": {},
   "source": [
    "# PREDICT WITH THE NET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465fc086",
   "metadata": {},
   "source": [
    "As we have an open net, we can use it directly in the function predict_gen, but we can also create a net loading one of the checkpoints if we jumped the cells in 'TRAIN&VALID ONE EPOCH' and 'TRAIN NET COMLPETELY' sections but we have saved checkpoints from other day. For example in my case would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8316075",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_weights = '/lhome/ext/ific020/ific0201/NEXT_SPARSECONVNET/examples/net_savings/checkpoints/net_checkpoint_2.pth.tar'\n",
    "\n",
    "#uncommment this to read one of your saved checkpoints\n",
    "\n",
    "#dct_weights = torch.load(saved_weights)['state_dict']\n",
    "#net.load_state_dict(dct_weights, strict=False)\n",
    "#print('weights loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d508ffc",
   "metadata": {},
   "source": [
    "For the prediction we choose the file to predict (usually is the validation file), the batch size for performin the prediction and the number of events to predict. I will use all the same as for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a278f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = predict_gen(data_path = valid_path,\n",
    "                  label_type = label_type,\n",
    "                  net = net,\n",
    "                  batch_size = batch_size_valid,\n",
    "                  nevents = nevents_valid, \n",
    "                  use_cuda = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e3a9b18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': array([0, 0]), 'dataset_id': tensor([0, 1], dtype=torch.int32), 'prediction': array([[0.7631363 , 0.23686369],\n",
      "       [0.69393784, 0.30606216]], dtype=float32)}\n",
      "{'label': array([0, 0]), 'dataset_id': tensor([2, 3], dtype=torch.int32), 'prediction': array([[0.61775494, 0.38224503],\n",
      "       [0.6239486 , 0.37605146]], dtype=float32)}\n",
      "{'label': array([0, 1]), 'dataset_id': tensor([4, 5], dtype=torch.int32), 'prediction': array([[0.6939479 , 0.30605212],\n",
      "       [0.6611927 , 0.3388073 ]], dtype=float32)}\n",
      "{'label': array([1, 0]), 'dataset_id': tensor([6, 7], dtype=torch.int32), 'prediction': array([[0.6937557 , 0.30624425],\n",
      "       [0.6496358 , 0.3503642 ]], dtype=float32)}\n",
      "{'label': array([1, 0]), 'dataset_id': tensor([8, 9], dtype=torch.int32), 'prediction': array([[0.729201 , 0.270799 ],\n",
      "       [0.7206646, 0.2793354]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "for dct in gen:\n",
    "    print(dct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8d669d",
   "metadata": {},
   "source": [
    "As we have 10 events in batches of 2 events, we will have 5 batches. If we look to what each batch returns, we see that we have the label of the event, the dataset_id of the event, and the prediction for each event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a9311b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': array([1, 0]),\n",
       " 'dataset_id': tensor([8, 9], dtype=torch.int32),\n",
       " 'prediction': array([[0.729201 , 0.270799 ],\n",
       "        [0.7206646, 0.2793354]], dtype=float32)}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297c482d",
   "metadata": {},
   "source": [
    "For example, the event 8 was signal (label 1) and have a prediction of [0.729201 , 0.270799 ], which means that it has a probability of 73% to be class 0 and 27% to be class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3c09f4",
   "metadata": {},
   "source": [
    "For the segnet, the main difference would be that in the prediction we will have 3 values, as we have 3 classes, and the prediction will be done FOR EACH VOXEL of the events in the batch, so we would also have the information of the coordinates of the voxels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0fc0d9",
   "metadata": {},
   "source": [
    "The rest of the main script is for arranging these results in a dataframe and writing it into a new file, defined in the out_file parameter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
